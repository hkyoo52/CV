## 앙상블
* 서로 다른 여러 학습 모델을 사용하는 것
* 일반적으로 딥러닝은 training fitting 되도록 학습하기 때문에 Overfitting될 가능성이 높다.
* 각각의 모델이 특성을 띄고 있을 때 성능이 좋다.
* 서로 다른 모델이 서로 다른 것에 에러를 만들기 때문에 성능이 좋다.

#### 배깅
* 부트스트랩 : n개의 훈련 집합이 있을 때 m개의 복원 추출 방법과 균등 확률분포를 이용해서 n'크기를 갖는 새로운 집합을 생성한다.
* 부트스트랩 샘플들을 이용하여 다양한 모델을 가지고 평균을 낸다.

#### 부스틍
*~


## 보팅 방식
#### 하드 보팅
* 모델들의 결과의 다수결 투표를 통해 답을 구함

#### 소프트 보팅
* 각 클래스가 나올 확률들의 평균으로 값을 구함


## 데이터셋 나누기
#### Cross Validation

![image](https://user-images.githubusercontent.com/63588046/155926379-8bc2e313-f6a7-4de4-bf00-be89081c38d5.png)

* training 데이터를 줄임 -> bias 감소

#### K-Fold Cross Validation

![image](https://user-images.githubusercontent.com/63588046/155926699-ad6b79ef-266a-4383-ada2-d991d3db9005.png)

* K가 너무 작으면 일반화가 잘 안된다.
* K가 너무 크면 너무 시간이 오래 걸린다.

## TTA(Test Time Augmentation)
* 노이즈를 섞인 것도 동일한 값이 나올까?
* 동일한 물체에 노이즈를 주어서 나온 값들을 가지고 앙상블을 진행한다!!
![image](https://user-images.githubusercontent.com/63588046/155927048-458e7d90-4405-4f3e-9b16-68919a6f1fd2.png)

## Hyperparameter Optimizer
* 계속해서 hyperparameter를 변경시켜서 최적의 하이퍼파라미터를 찾는다.
* 가장 많이 쓰이는 방식 : 베이지안 optimizer



## Tensorboard
* 학습 하는 도중에 계속해서 알 수 있다.
* tensorboard --logdir PATH --hostADDR --port PORT

## wandb
