
# 메타러닝
* 사람처럼 모델이 생각하도록 만드는 것
* 사람은 한번에 classfication, detection, segmentation 다 잘함. 근데 모델은 1개만 잘함... 그래서 전부 잘하게 하고 싶음
* 주요 논문 : MAML (자코비안 행렬을 또 미분을 해서 해시안 행렬을 만든다. 근데 해시안 행렬이 0이어도 학습을 잘한다?)

## MAML

## Continual Learning
* 연속적으로 학습을 하면 학습 성능이 안좋아진다.
* Ex. 모델을 MNIST로 학습 -> 모들을 CIFAR10으로 학습 -> MNIST로 성능을 보면 성능이 확 떨어짐
* 이 현상을 catastrophic forgetting이라고 부름

#### 해결법1 메모리 사용
* 이전에 학습한 데이터의 일부를 들고 다님
* CIFAR10 학습할 때 MNIST의 일부도 같이 학습

#### 해결법2 GAN 사용
* 새로운 데이터를 학습할 때 이전 데이터를 생성해서 이후 학습때 같이 학습


## 모듈화

![image](https://user-images.githubusercontent.com/63588046/166609444-47ea74b1-a343-4aad-bb05-c121ab772cb0.png)
